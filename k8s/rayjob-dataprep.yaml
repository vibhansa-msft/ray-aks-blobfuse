# ========== RAY DATA PREPARATION JOB ==========
# This RayJob specification configures a Ray cluster for data preparation tasks
# Designed to run various data preparation scripts (prepare_data.py)
#
# Key features:
#   - CPU-only worker nodes optimized for data processing
#   - Azure Blob Storage integration for data output
#   - Distributed Ray cluster for parallel data processing
#   - Flexible dataset download and Parquet conversion

apiVersion: ray.io/v1
kind: RayJob

# ========== METADATA ==========

metadata:
  name: data-prep-job

# ========== RAY JOB SPECIFICATION ==========

spec:
  # ===== Job Configuration =====
  
  # Shutdown the Ray cluster when job finishes
  shutdownAfterJobFinishes: true
  
  # TTL for automatic cleanup - delete job 30 minutes after completion
  ttlSecondsAfterFinished: 1800

  # Entry point command - can be customized for different datasets
  entrypoint: "python /app/prepare_data.py"  # Runtime environment variables and dependencies
  runtimeEnvYAML: |
    pip:
      - datasets==2.16.0
      - pyarrow==14.0.0
      - transformers==4.36.0
      - pandas>=1.3.0
    env_vars:
      DATA_DIR: "__DATA_DIR__"
      CHECKPOINT_DIR: "__CHECKPOINT_DIR__"
      PYTHONPATH: "__APP_DIR__"

  # ========== RAY CLUSTER SPECIFICATION ==========

  rayClusterSpec:
    # Ray version - uses official Ray image from Docker Hub
    rayVersion: "2.50.0"

    # ===== HEAD NODE CONFIGURATION =====
    # Single head node that coordinates the distributed cluster
    
    headGroupSpec:
      serviceType: ClusterIP
      # Enable Ray Dashboard
      rayStartParams:
        dashboard-host: "0.0.0.0"
        include-dashboard: "true"
        dashboard-port: "8265"
      template:
        spec:
          initContainers:
          - name: setup-blobfuse-cache
            image: busybox:1.28
            command: ['mkdir', '-p', '__CACHE_DIR__']
            resources:
              requests:
                cpu: "100m"
                memory: "128Mi"
              limits:
                cpu: "200m"
                memory: "256Mi"
            volumeMounts:
              - name: cache-dir
                mountPath: /mnt
          containers:
          - name: ray-head
            # Standard Ray image with Python support
            image: rayproject/ray:2.50.0
            
            # Resource requests for proper scheduling and autoscaling
            resources:
              requests:
                cpu: "4000m"      # 4 CPU cores for head node
                memory: "8Gi"     # 8GB memory for head node
              limits:
                cpu: "8000m"      # Max 8 CPU cores
                memory: "16Gi"    # Max 16GB memory
            
            # Ray object spilling configuration for memory management
            env:
              - name: RAY_OBJECT_SPILLING_CONFIG
                value: |
                  {"type":"filesystem","params":{"directory_path":"/tmp/ray_spill"}}
            
            # Mount Azure Blob Storage for dataset output
            volumeMounts:
              - name: blob-dataset
                mountPath: "__DATA_DIR__"
              - name: blob-checkpoints
                mountPath: "__CHECKPOINT_DIR__"
              - name: app-code
                mountPath: "__APP_DIR__"
              - name: cache-dir
                mountPath: "__CACHE_DIR__"
          
          # Persistent volumes for Azure Blob Storage
          volumes:
            - name: blob-dataset
              persistentVolumeClaim:
                claimName: blob-pvc-dataset
            - name: blob-checkpoints
              persistentVolumeClaim:
                claimName: blob-pvc-checkpoint
            - name: app-code
              configMap:
                name: hpo-app-code
                defaultMode: 0755
            - name: cache-dir
              emptyDir: {}

    # ===== CPU WORKER NODES CONFIGURATION =====
    # Worker nodes optimized for data processing tasks
    
    workerGroupSpecs:
    - groupName: data-workers
      replicas: __WORKER_REPLICAS__
      template:
        spec:
          initContainers:
          - name: setup-blobfuse-cache
            image: busybox:1.28
            command: ['mkdir', '-p', '__CACHE_DIR__']
            resources:
              requests:
                cpu: "100m"
                memory: "128Mi"
              limits:
                cpu: "200m"
                memory: "256Mi"
            volumeMounts:
              - name: cache-dir
                mountPath: /mnt
          containers:
          - name: ray-worker
            # Standard Ray image with Python support
            image: rayproject/ray:2.50.0
            
            # Resource requests for proper scheduling and autoscaling
            resources:
              requests:
                cpu: "10000m"     # 10 CPU cores per worker (Standard_D48ds_v5 has 48 cores)
                memory: "32Gi"    # 80GB memory per worker (Standard_D48ds_v5 has 192GB)
              limits:
                cpu: "12000m"     # Max 12 CPU cores per worker
                memory: "40Gi"   # Max 100GB memory per worker (for data processing)
            
            # Ray object spilling for memory management
            env:
              - name: RAY_OBJECT_SPILLING_CONFIG
                value: |
                  {"type":"filesystem","params":{"directory_path":"/tmp/ray_spill"}}
            
            # Mount Azure Blob Storage for dataset output
            volumeMounts:
              - name: blob-dataset
                mountPath: "__DATA_DIR__"
              - name: blob-checkpoints
                mountPath: "__CHECKPOINT_DIR__"
              - name: app-code
                mountPath: "__APP_DIR__"
              - name: cache-dir
                mountPath: "__CACHE_DIR__"
          
          # Persistent volumes for Azure Blob Storage
          volumes:
            - name: blob-dataset
              persistentVolumeClaim:
                claimName: blob-pvc-dataset
            - name: blob-checkpoints
              persistentVolumeClaim:
                claimName: blob-pvc-checkpoint
            - name: app-code
              configMap:
                name: hpo-app-code
                defaultMode: 0755
            - name: cache-dir
              emptyDir: {}