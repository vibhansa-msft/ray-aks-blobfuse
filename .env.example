# Storage details
# Subscription id, resource group name, location, AKS cluster name, storage account name and container name
SUBSCRIPTION="68e5d74d-cc6b-4be6-9606-cd1c77fa55f0"
RG="vibhansa-ray-aks-rg"
LOC="swedencentral"
AKS="vibhansa-ray-aks"
SA="vibhansaraystorage"
CONTAINER="datasets"

# Cluster Configuration
# GPU based cluster or not, node count, cpu or gpu based node type
GPU=false
NodeCount=14
VmType="Standard_D48ds_v5"
#VmType="Standard_D4_v2"
#VmType="Standard_NC6s_v3"

# Ray Configuration
# Number of workers in the pool and number of nodes to be used to run that pool
# IMPORTANT: With 2-node cluster:
#   - WORKER_REPLICAS should be 2-3 (one per node + 1 for head node)
#   - NUM_WORKERS should be 4-6 (depends on available CPU/RAM per node)
#   - Default: 2 replicas Ã— 4 workers = 8 total workers (fits in 2 nodes)
WORKER_REPLICAS=40
NUM_WORKERS=40

# Mount Path Configuration
# These paths define where storage will be mounted in Ray containers
# Customize these paths based on your application requirements
DATA_DIR="/mnt/blob/datasets"
CHECKPOINT_DIR="/mnt/blob/checkpoints"
APP_DIR="/app"
CACHE_DIR="/mnt/blobfusecache"

# Preprocessing Configuration
#
# Change to "k8s/rayjob-dataprep-adlfs.yaml" for ADLFS-based preprocessing
DATAPREP_RAY_CONFIG="k8s/rayjob-dataprep.yaml"
# ADLFS specific preprocessing configurations
#
# ADLFS setup uses account key for authentication. Add account key here before running Ray job
STORAGE_ACCOUNT_KEY=""
# The number of preprocess tasks running concurrently at each stage of the data pipeline. On average each,
# .arrow file is +200MB and there are 3 stages in the pipeline (read, convert to arrow table, write) where each input
# and output will be roughly the same size. Setting  a max concurrency to 2 will mean there will be no more than 2 read,
# 2 convert, and 2 write tasks running at once, helping to limit memory overall memory usage.
MAX_PREPROCESS_TASK_CONCURRENCY=2
